# 6. Deploy the Retrieval Service to Cloud Run

Now that the database is initialized, we'll deploy a retrieval service as a containerized application on Cloud Run. This service will expose API endpoints to perform RAG operations using LangChain, interacting with your AlloyDB AI database and Vertex AI models.

All commands in this section should be run from your **Google Cloud Shell**, not the GCE VM (unless specified). Ensure your `PROJECT_ID` and `REGION` environment variables are set.

## 1. Understanding the Retrieval Service

The retrieval service is a Python FastAPI application with the following key components:

*   **API Endpoints:**
    *   `/retrieve`: Takes a query and returns relevant documents from AlloyDB.
    *   `/rag_answer`: Takes a query, retrieves relevant documents, and uses a Vertex AI LLM to generate an answer based on those documents.
    *   `/add_texts`: (Example) Allows adding new texts to the vector store.
    *   `/`: Basic health check.
*   **LangChain Integration:**
    *   **`VertexAIEmbeddings`**: Used to generate embeddings for input queries and potentially for documents if not generated by AlloyDB AI directly.
    *   **`PGVector`**: The LangChain vector store component that interfaces with your AlloyDB database (using the `pgvector` extension and the tables you created). It handles storing and querying vector embeddings.
    *   **`ChatVertexAI`**: The LangChain component for interacting with generative models on Vertex AI (e.g., Gemini).
    *   **Retrieval Chain (`rag_chain`)**: An LCEL (LangChain Expression Language) chain that:
        1.  Takes a user's question.
        2.  Retrieves relevant document chunks from `PGVector`.
        3.  Formats a prompt including the retrieved context and the original question.
        4.  Sends the prompt to the `ChatVertexAI` LLM.
        5.  Returns the LLM's answer.
*   **Database Interaction:** The service connects to your AlloyDB instance using connection details provided via environment variables.

## 2. Service Code Setup

For this lab, we've provided the necessary code files. In your Cloud Shell, navigate to the lab directory. You should have already created the following files in the `alloydb-langchain-rag-lab/retrieval-service/` sub-directory in the previous steps:

*   **`main.py`**: The FastAPI application code. (Content was provided in the previous step).
*   **`requirements.txt`**: Lists the Python dependencies. (Content was provided in the previous step).
*   **`Dockerfile`**: Instructions to build the Docker container image. (Content was provided in the previous step).

Ensure these files are present in `alloydb-langchain-rag-lab/retrieval-service/`. You can use the Cloud Shell editor to review or modify them if needed.

## 3. Create Artifact Registry Repository

Google Artifact Registry is used to store and manage container images.

*   Define a name for your Docker repository:
    ```bash
    export AR_REPO_NAME="alloydb-rag-service-repo"
    echo "Artifact Registry Repo Name: ${AR_REPO_NAME}"
    ```
*   Create the repository in the same region as your other services:
    ```bash
    gcloud artifacts repositories create ${AR_REPO_NAME} \
        --repository-format=docker \
        --location=${REGION} \
        --description="Docker repository for AlloyDB RAG Lab service" \
        --project=${PROJECT_ID}
    ```
    You should see a success message.

## 4. Build the Docker Container

Now, build the Docker image using Cloud Build and push it to your Artifact Registry repository.

*   Navigate to the service code directory in your Cloud Shell:
    ```bash
    cd alloydb-langchain-rag-lab/retrieval-service
    ```
*   Define the image URI:
    ```bash
    export IMAGE_URI="${REGION}-docker.pkg.dev/${PROJECT_ID}/${AR_REPO_NAME}/alloydb-rag-service:latest"
    echo "Image URI: ${IMAGE_URI}"
    ```
*   Start the build process:
    ```bash
    gcloud builds submit --tag ${IMAGE_URI} . --project=${PROJECT_ID}
    ```
    This command uploads the contents of the current directory (including the Dockerfile) to Cloud Build, builds the image, and then pushes it to the specified URI in Artifact Registry. This process will take a few minutes. You'll see logs from the build process in your terminal.

## 5. Deploy to Cloud Run

With the container image built and stored, deploy it to Cloud Run.

*   **Create a Service Account for Cloud Run:**
    It's best practice for each service to have its own dedicated service account with the minimum necessary permissions.
    ```bash
    export CLOUD_RUN_SA_NAME="cloud-run-rag-sa"
    echo "Cloud Run Service Account Name: ${CLOUD_RUN_SA_NAME}"

    gcloud iam service-accounts create ${CLOUD_RUN_SA_NAME} \
        --description="Service Account for Cloud Run RAG Service" \
        --display-name="Cloud Run RAG SA" \
        --project=${PROJECT_ID}
    ```
*   **Grant Necessary Permissions to the Cloud Run Service Account:**
    *   To invoke Vertex AI (for embeddings and LLM):
        ```bash
        gcloud projects add-iam-policy-binding ${PROJECT_ID} \
            --member="serviceAccount:${CLOUD_RUN_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
            --role="roles/aiplatform.user"
        ```
    *   To connect to AlloyDB (as a client):
        ```bash
        gcloud projects add-iam-policy-binding ${PROJECT_ID} \
            --member="serviceAccount:${CLOUD_RUN_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
            --role="roles/alloydb.client"
        ```
    *   (Optional) If your service needs to access secrets from Secret Manager:
        ```bash
        # gcloud projects add-iam-policy-binding ${PROJECT_ID} \
        #     --member="serviceAccount:${CLOUD_RUN_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
        #     --role="roles/secretmanager.secretAccessor"
        ```

*   **Create a Serverless VPC Access Connector (If not already created):**
    Cloud Run needs a Serverless VPC Access connector to communicate with resources in your VPC network, like your AlloyDB instance, that do not have public IP addresses.
    ```bash
    export VPC_CONNECTOR_NAME="rag-lab-connector"
    echo "VPC Connector Name: ${VPC_CONNECTOR_NAME}"

    # Check if it exists, create if not
    gcloud compute networks vpc-access connectors describe ${VPC_CONNECTOR_NAME} --region=${REGION} --project=${PROJECT_ID} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "Creating VPC Access Connector: ${VPC_CONNECTOR_NAME}..."
      gcloud compute networks vpc-access connectors create ${VPC_CONNECTOR_NAME} \
          --region=${REGION} \
          --network=default \
          --range=10.8.0.0/28 \
          --project=${PROJECT_ID}
          # The IP range must not overlap with existing subnets in your VPC.
          # This operation can take several minutes.
    else
      echo "VPC Access Connector ${VPC_CONNECTOR_NAME} already exists."
    fi
    ```

*   **Deploy the Service:**
    Retrieve your AlloyDB instance IP and password if you don't have them readily available.
    ```bash
    # Ensure these are set from previous steps or retrieve them
    # export ADB_INSTANCE_IP="YOUR_ALLOYDB_INSTANCE_IP" (from step 03)
    # export PGPASSWORD="YOUR_ALLOYDB_PASSWORD" (from step 03)
    # export ADB_DATABASE="rag_lab_db" (from step 05)
    # export ADB_USER="postgres" (or your custom user from step 05)

    # Ensure ADB_INSTANCE_IP is set (e.g., from step 03)
    if [ -z "${ADB_INSTANCE_IP}" ]; then
      export ADB_INSTANCE_IP=$(gcloud alloydb instances describe ${ADBINSTANCE} --cluster=${ADBCLUSTER} --region=${REGION} --project=${PROJECT_ID} --format="value(ipAddress)")
    fi
    
    echo "Deploying with DB_HOST=${ADB_INSTANCE_IP}, DB_USER=${ADB_USER}, DB_NAME=${ADB_DATABASE}"

    gcloud run deploy alloydb-rag-service \
        --image=${IMAGE_URI} \
        --platform=managed \
        --region=${REGION} \
        --service-account=${CLOUD_RUN_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \
        --no-allow-unauthenticated \
        --vpc-connector=${VPC_CONNECTOR_NAME} \
        --set-env-vars="DB_HOST=${ADB_INSTANCE_IP},DB_PORT=5432,DB_USER=${ADB_USER},DB_PASSWORD=${PGPASSWORD},DB_NAME=${ADB_DATABASE},EMBEDDING_MODEL_NAME=textembedding-gecko@latest,LANGUAGE_MODEL_NAME=gemini-pro,COLLECTION_NAME=documents,PROJECT_ID=${PROJECT_ID},REGION=${REGION}" \
        --project=${PROJECT_ID}
        # Adjust CPU, memory, concurrency as needed with flags like --cpu, --memory, --concurrency
    ```
    This command deploys your container to Cloud Run.
    *   `--no-allow-unauthenticated`: Ensures your service requires authentication.
    *   `--vpc-connector`: Connects your Cloud Run service to your VPC.
    *   `--set-env-vars`: Sets necessary environment variables for the application. **Replace `YOUR_ALLOYDB_PASSWORD` with the actual password if not using the PGPASSWORD variable directly.** It's more secure to use Secret Manager for passwords in production.

    The deployment process will take a few minutes. Once complete, it will output the Service URL.

## 6. Verify the Service

*   Get the URL of your deployed service:
    ```bash
    export SERVICE_URL=$(gcloud run services describe alloydb-rag-service --platform=managed --region=${REGION} --project=${PROJECT_ID} --format="value(status.url)")
    echo "Service URL: ${SERVICE_URL}"
    ```

*   Test the health check endpoint (does not require authentication for this specific path if you configure it that way, but by default, all paths are protected):
    ```bash
    # To invoke, you need an identity token for an account with roles/run.invoker
    # Your Cloud Shell identity usually has this for services in the same project.
    curl -H "Authorization: Bearer $(gcloud auth print-identity-token)" ${SERVICE_URL}/
    ```
    You should see `{"status":"ok"}`.

*   Test the `/rag_answer` endpoint (example):
    ```bash
    curl -X POST -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
         -H "Content-Type: application/json" \
         -d '{"question": "What is AlloyDB?"}' \
         ${SERVICE_URL}/rag_answer
    ```
    This will send a POST request to your service. If everything is set up correctly (including data in your AlloyDB tables, which we'll cover in a subsequent section on data loading/ingestion), you should get a JSON response with an answer. If no data is loaded yet, it might return an answer based on the LLM's general knowledge or an empty context message.

Your RAG retrieval service is now deployed on Cloud Run. The next step would typically involve populating your AlloyDB database with documents and their embeddings so the RAG service can retrieve them.Okay, I have created the `main.py`, `requirements.txt`, and `Dockerfile` for the retrieval service and populated `06-deploy-retrieval-service.md` with the deployment steps.

The markdown file now includes:
1.  **Introduction to the Retrieval Service**: Explains its purpose and core technologies.
2.  **Service Code Setup**: Refers to the `main.py`, `requirements.txt`, and `Dockerfile` that were created in previous turns.
3.  **Create Artifact Registry Repository**: Commands to create a Docker repository.
4.  **Build the Docker Container**: Commands to build the image using Cloud Build and push it to Artifact Registry.
5.  **Deploy to Cloud Run**:
    *   Instructions for creating a dedicated Cloud Run service account and granting it `aiplatform.user` and `alloydb.client` roles.
    *   Instructions for creating a Serverless VPC Access Connector, including a check if it already exists.
    *   The `gcloud run deploy` command, including:
        *   Image URI.
        *   Service account.
        *   Region.
        *   `--no-allow-unauthenticated`.
        *   `--vpc-connector`.
        *   Crucial environment variables for database connection (`DB_HOST`, `DB_USER`, `DB_PASSWORD`, `DB_NAME`), AI models (`EMBEDDING_MODEL_NAME`, `LANGUAGE_MODEL_NAME`), and GCP settings (`PROJECT_ID`, `REGION`).
6.  **Verify the Service**:
    *   Command to get the service URL.
    *   `curl` commands to test the health check and `/rag_answer` endpoints, including how to pass an identity token.

The content is structured based on the template lab but adapted for the specific LangChain, AlloyDB AI, and FastAPI stack, providing a comprehensive guide for deploying the retrieval service.

I will now submit the subtask report.
